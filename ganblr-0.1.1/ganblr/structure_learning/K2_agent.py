import random
from collections import deque
from itertools import permutations

import networkx as nx
import numpy
import torch as T
import numpy as np
from .utils.buffer import ReplayBuffer
from warnings import simplefilter

from pgmpy.models import BayesianNetwork
from pgmpy.metrics import log_likelihood_score
from pgmpy.estimators import (
    AICScore,
    BDeuScore,
    BDsScore,
    BicScore,
    K2Score,
    ScoreCache,
    StructureEstimator,
    StructureScore,
)

from sklearn.metrics import mutual_info_score

simplefilter(action="ignore", category=FutureWarning)

device = T.device("cuda:0" if T.cuda.is_available() else "cpu")

if T.backends.mps.is_available():
    device = T.device("mps")


class K2Agent:
# This class should include the K2 Algorithm and the memorizing functions
    def __init__(self, alpha=0.1, gamma=0.9, epsilon=0.3, max_size=1000000, data_X=None, data_Y=None):
        #gamma 0.8 0.9

        # Q(S,A) + a*(r*maxQ(S',A')-Q(S,A))
        self.alpha = alpha  # The Learning Rate
        self.gamma = gamma  # The Discount Factor
        self.epsilon = epsilon  # The Possibility for Exploration
        self.max_size = max_size  # The max_size for Q-table
        self.X = data_X  # The dataset for the Bayesian Network Learning
        self.Y = data_Y  # The label for the dataset for the Bayesian Network Learning
        self.variables = list(self.X.columns.values) # The variables in the Baysian Network
        self.done_bonus = 0.3 # The bonus of finishing forming up the Bayesian Structural Learning

        self.tabu_list = deque(maxlen=1000)

        self.current_model = BayesianNetwork() #就是Bayesian Network，但是要使用特定的node ordering进行观察
        self.current_model.add_nodes_from(self.variables)

        self.node_list = self.ordering_nodes()

        # The Q-Table: Memorize the experience of Q(S,A)： S is the networkx object, the A is generated by choose_action
        self.Q_table = numpy.ndarray(len(self.node_list),len(self.node_list))

    def _init_table(self):
        # for i in range(len(self.node_list)):
        #     for j in range(j,len(self.node_list)):
        #         self.Q_table[i,j] = np.nan

        self.Q_table = np.triu(self.Q_table, 0)
        np.fill_diagonal(self.Q_table, np.nan)

        for i in range(len(self.node_list)): # As column
            non_nan_indices = ~np.isnan(self.Q_table[i])
            non_nan_sum = np.nansum(self.Q_table[i])
            if non_nan_sum != 0:
                self.Q_table[i, non_nan_indices] = self.Q_table[i, non_nan_indices] / non_nan_sum

    def _available_actions(self,graph):#the graph should be a Bayesian Network
        available_list = []
        for i,X in enumerate(self.variables):
            for Y in self.variables[i+1:]:
                if graph.has_edge(X,Y):
                    available_list.append("-",X,Y)
                else:
                    available_list.append("+",X,Y)

        return available_list

    def _choose_action(self, graph, exploration = True, ): # if the exploration is false, then it will be a greedy approach
        # The observation is ordered_node, containing the "action taken", and statevisited
        # Greedy: if the graph in Q-table, find the highest value
        # No Exploration is pure greedy

        #Use the available_action_list
        available_action_list = self._available_actions(graph)

        # Exploitation
        best_operation = random.choice(available_action_list)
        best_score_delta = None

        # for (s,a),q_value in self.Q_table.items():
        #     if s == graph:
        #         if q_value > best_score_delta:
        #             best_score_delta = q_value
        #             best_operation = a

        # Epsilon: a random action in the available list
        if (np.random.random() < 0.3 and exploration) or (best_operation == None):
            action_list = []
            for i in self._legal_operations(
                graph,
                score_fn,
                score.structure_prior_ratio,
            ):
                action_list.append(i)
            if (len(action_list) > 0):
                random_index = random.randint(0,len(action_list)-1)
                best_operation, best_score_delta = action_list[random_index]
            else:
                best_operation, best_score_delta = (None,None)


        return best_operation, best_score_delta

    def ordering_nodes(self):
        mutual_info_dict = {}
        for node in self.variables:
            mi_score = mutual_info_score(self.X[node],self.Y)
            mutual_info_dict[node] = mi_score

        ordering = list(sorted(mutual_info_dict.items(), key=lambda item: item[1], reverse=True))

        return ordering

    def estimate_once(self, start_dag):
        #self.current_model is the dag model to start with
        #self.best_operation is the newly added node from the maximization function
        best_operation = self._choose_action(graph=start_dag)
        #Edit the current_model

        return self.current_model, best_operation #这里的current_model本身就需要是Bayesian Network的形式